Action space: [Discrete(3), Discrete(3)]
Observation (state) space: [Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (10,), float32), Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (10,), float32)]
Episode #349 Rewards: [6, -6]
Wins - losses: 494
Gamma:0.75
Epsilon: 0.1
Min epsilon: 0.01
Alpha: 0.1
Q table size: 9749
Last 10 games: 20
Action space: [Discrete(3), Discrete(3)]
Observation (state) space: [Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (10,), float32), Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (10,), float32)]
Episode #349 Rewards: [6, -6]
Wins - losses: 470
Gamma:0.75
Epsilon: 0.2
Min epsilon: 0.01
Alpha: 0.1
Q table size: 9912
Last 10 games: 28
Action space: [Discrete(3), Discrete(3)]
Observation (state) space: [Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (10,), float32), Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (10,), float32)]
